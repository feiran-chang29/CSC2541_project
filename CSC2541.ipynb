{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyrMWtlUSFmG",
        "outputId": "26509b2a-85bb-4b2f-806d-5b436ccbefb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 0. Load drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install dependency\n",
        "\n",
        "!pip install -q git+https://github.com/dccastro/Morpho-MNIST.git\n",
        "!pip install -q causalpfn torch torchvision tqdm pandas scikit-learn\n",
        "!pip install -q timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9uzmlRvSMx5",
        "outputId": "920c8e77-12fb-4c90-86fd-84899c6c3f5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for Morpho-MNIST (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from morphomnist import measure\n",
        "from causalpfn import CATEEstimator, ATEEstimator\n",
        "\n",
        "import timm\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbT8ciAqSUeb",
        "outputId": "47a35014-c703-4e94-e059-70c0317b93ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load Dino & MNIST\n",
        "\n",
        "# Dino\n",
        "dino_model_name = \"vit_small_patch16_224.dino\"\n",
        "\n",
        "dino = timm.create_model(\n",
        "    dino_model_name,\n",
        "    pretrained=True,\n",
        ")\n",
        "\n",
        "dino.reset_classifier(0)\n",
        "dino = dino.to(device)\n",
        "dino.eval()\n",
        "\n",
        "print(\"Loaded DINO model:\", dino_model_name)\n",
        "\n",
        "# MNIST\n",
        "metrics_path = \"/content/drive/MyDrive/morpho_metrics_train.csv\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "images = mnist_train.data.numpy().astype(np.uint8)          # (60000, 28, 28)\n",
        "labels = mnist_train.targets.numpy().astype(np.int64)       # (60000,)\n",
        "\n",
        "print(\"Images shape:\", images.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "if os.path.exists(metrics_path):\n",
        "    print(\"Reading Morpho-MNIST metrics from drive：\", metrics_path)\n",
        "    metrics_df = pd.read_csv(metrics_path)\n",
        "else:\n",
        "    print(\"No downloaded metrics，start calculating Morpho-MNIST metrics ...\")\n",
        "    metrics_df = measure.measure_batch(images)\n",
        "    metrics_df.to_csv(metrics_path, index=False)\n",
        "    print(\"Finish calculation, save to：\", metrics_path)\n",
        "\n",
        "print(metrics_df.head())\n",
        "\n",
        "thickness = metrics_df[\"thickness\"].to_numpy().astype(np.float32)\n",
        "slant     = metrics_df[\"slant\"].to_numpy().astype(np.float32)\n",
        "\n",
        "intensity = images.reshape(len(images), -1).mean(axis=1).astype(np.float32)\n",
        "\n",
        "print(\"thickness shape:\", thickness.shape)\n",
        "print(\"slant shape:\", slant.shape)\n",
        "print(\"intensity shape:\", intensity.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "37e55536bbd04c82a3896bdd3a53f646",
            "70ce52fdc6d04a1a8d28a9671de49060",
            "4867153020ec4119b685afdc98394c5c",
            "5889d66e21084ed0a3fd0316301f935f",
            "0418802e25294b878bb01579ebda7fba",
            "b07dfdcbcdb3414dbf1fea8ad9f03ead",
            "efe4754638794100bc731637815dc82a",
            "48ef5160be29465d91e6565972ad2c83",
            "4c4b925ca6124659a61b571081255544",
            "66b8c00d6f744d3db2b7d68970ec723d",
            "fd4844936db549959e35e40fb1403506"
          ]
        },
        "id": "-Cp-FlOfSpzx",
        "outputId": "e74457da-5396-4ffd-86eb-26c9d4f44c4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37e55536bbd04c82a3896bdd3a53f646"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded DINO model: vit_small_patch16_224.dino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 610kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.57MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.50MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: (60000, 28, 28)\n",
            "Labels shape: (60000,)\n",
            "Reading Morpho-MNIST metrics from drive： /content/drive/MyDrive/morpho_metrics_train.csv\n",
            "       area     length  thickness     slant      width     height\n",
            "0  107.1875  47.748737   2.525400  0.231577  14.523651  19.889152\n",
            "1  123.0625  52.905592   2.570187  0.330892  15.385168  19.359169\n",
            "2   78.1875  47.109650   1.853382 -0.213113  20.472912  19.133937\n",
            "3   67.3750  23.202796   2.922822  0.507930   5.406406  19.768806\n",
            "4   91.0000  46.223611   2.244745  0.008993  13.273560  19.492584\n",
            "thickness shape: (60000,)\n",
            "slant shape: (60000,)\n",
            "intensity shape: (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Extract image embeddings\n",
        "\n",
        "dino_embed_path = \"/content/drive/MyDrive/mnist_dino_embeds.npy\"\n",
        "\n",
        "class MNISTForDINO(Dataset):\n",
        "    def __init__(self, images_np):\n",
        "        \"\"\"\n",
        "        images_np: (N, 28, 28) uint8\n",
        "        \"\"\"\n",
        "        self.images = images_np\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "            transforms.ToTensor(),  # [0,1]\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std =[0.229, 0.224, 0.225],\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "if os.path.exists(dino_embed_path):\n",
        "    print(\"Loading extracted embeddings from:\", dino_embed_path)\n",
        "    dino_embs = np.load(dino_embed_path)\n",
        "else:\n",
        "    print(\"No downloaded DINO embedding, start extracting...\")\n",
        "\n",
        "    dataset_dino = MNISTForDINO(images)\n",
        "    loader_dino = DataLoader(\n",
        "        dataset_dino,\n",
        "        batch_size=128,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    all_embs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader_dino, desc=\"Extracting DINO embeddings\"):\n",
        "            batch = batch.to(device)\n",
        "            feats = dino(batch)          # (B, d)\n",
        "            if isinstance(feats, (tuple, list)):\n",
        "                feats = feats[0]\n",
        "            all_embs.append(feats.cpu())\n",
        "\n",
        "    dino_embs = torch.cat(all_embs, dim=0).numpy().astype(np.float32)\n",
        "    print(\"DINO embeddings shape:\", dino_embs.shape)\n",
        "\n",
        "    np.save(dino_embed_path, dino_embs)\n",
        "    print(\"Finish extracting DINO embeddings, save to:\", dino_embed_path)\n",
        "\n",
        "print(\"DINO embeddings shape:\", dino_embs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMygBdiDVQUH",
        "outputId": "cb151e89-6b92-4803-8b37-3ccc29d83a17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading extracted embeddings from: /content/drive/MyDrive/mnist_dino_embeds.npy\n",
            "DINO embeddings shape: (60000, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Normalize + subsample\n",
        "\n",
        "N_TOTAL = len(images)\n",
        "N_SUBSAMPLE = 20000\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "indices = rng.choice(N_TOTAL, size=N_SUBSAMPLE, replace=False)\n",
        "\n",
        "digit     = labels[indices]\n",
        "phi_thick = thickness[indices]\n",
        "w_slant   = slant[indices]\n",
        "inten     = intensity[indices]\n",
        "dino_sub  = dino_embs[indices]\n",
        "\n",
        "def zscore(x):\n",
        "    return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "z_thick = zscore(phi_thick)\n",
        "z_slant = zscore(w_slant)\n",
        "z_inten = zscore(inten)\n",
        "\n",
        "print(\"Subsampled N:\", len(digit))\n",
        "print(\"dino_sub shape:\", dino_sub.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPJUse3JV7oE",
        "outputId": "0a6321c8-de8e-426b-dc37-b2794b9041fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subsampled N: 20000\n",
            "dino_sub shape: (20000, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Define World A / World B\n",
        "\n",
        "def simulate_world_A(z_thick, z_slant, seed=0):\n",
        "    \"\"\"\n",
        "    World A: only thickness is confounder,\n",
        "    T, Y only depends on z_thick。\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(z_thick)\n",
        "\n",
        "    gamma0, gamma1 = 0.0, 1.0\n",
        "    logits_T = gamma0 + gamma1 * z_thick\n",
        "    p_T = 1 / (1 + np.exp(-logits_T))\n",
        "    T = rng.binomial(1, p_T).astype(np.float32)\n",
        "\n",
        "    beta0, beta1 = 0.0, 1.0\n",
        "    tau0, tau1   = 0.5, 0.5\n",
        "    eps = rng.normal(0.0, 1.0, size=n).astype(np.float32)\n",
        "\n",
        "    Y0 = beta0 + beta1 * z_thick + eps\n",
        "    tau_true = tau0 + tau1 * z_thick\n",
        "    Y1 = Y0 + tau_true\n",
        "\n",
        "    Y = Y0 * (1 - T) + Y1 * T\n",
        "\n",
        "    return {\n",
        "        \"T\": T,\n",
        "        \"Y\": Y.astype(np.float32),\n",
        "        \"Y0\": Y0.astype(np.float32),\n",
        "        \"Y1\": Y1.astype(np.float32),\n",
        "        \"tau\": tau_true.astype(np.float32),\n",
        "    }\n",
        "\n",
        "\n",
        "def simulate_world_B(z_thick, z_slant, seed=0):\n",
        "    \"\"\"\n",
        "    World B: thickness and slant are confounders。\n",
        "    - T depends on thickness + slant\n",
        "    - Y0, τ(x) depends on thickness + slant\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(z_thick)\n",
        "\n",
        "    gamma0, gamma1, gamma2 = 0.0, 0.8, 1.2\n",
        "    logits_T = gamma0 + gamma1 * z_thick + gamma2 * z_slant\n",
        "    p_T = 1 / (1 + np.exp(-logits_T))\n",
        "    T = rng.binomial(1, p_T).astype(np.float32)\n",
        "\n",
        "    beta0, beta1, beta2 = 0.0, 1.0, 1.0\n",
        "    tau0, tau1, tau2    = 0.5, 0.5, 0.5\n",
        "    eps = rng.normal(0.0, 1.0, size=n).astype(np.float32)\n",
        "\n",
        "    Y0 = beta0 + beta1 * z_thick + beta2 * z_slant + eps\n",
        "    tau_true = tau0 + tau1 * z_thick + tau2 * z_slant\n",
        "    Y1 = Y0 + tau_true\n",
        "\n",
        "    Y = Y0 * (1 - T) + Y1 * T\n",
        "\n",
        "    return {\n",
        "        \"T\": T,\n",
        "        \"Y\": Y.astype(np.float32),\n",
        "        \"Y0\": Y0.astype(np.float32),\n",
        "        \"Y1\": Y1.astype(np.float32),\n",
        "        \"tau\": tau_true.astype(np.float32),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZVakWPurWGEp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Define input & World data\n",
        "\n",
        "X_tab = z_thick.reshape(-1, 1).astype(np.float32)\n",
        "X_img_dino = dino_sub.astype(np.float32)\n",
        "X_tab_img_dino = np.concatenate(\n",
        "    [z_thick.reshape(-1, 1).astype(np.float32), X_img_dino],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "worldA = simulate_world_A(z_thick, z_slant, seed=0)\n",
        "worldB = simulate_world_B(z_thick, z_slant, seed=1)\n",
        "\n",
        "T_A   = worldA[\"T\"]\n",
        "Y_A   = worldA[\"Y\"]\n",
        "tau_A = worldA[\"tau\"]\n",
        "\n",
        "T_B   = worldB[\"T\"]\n",
        "Y_B   = worldB[\"Y\"]\n",
        "tau_B = worldB[\"tau\"]\n",
        "\n",
        "ids_A = np.arange(len(T_A), dtype=int)\n",
        "ids_B = np.arange(len(T_B), dtype=int)\n",
        "\n",
        "print(\"N_A:\", len(T_A), \"N_B:\", len(T_B))\n",
        "print(\"X_tab shape:\", X_tab.shape)\n",
        "print(\"X_img_dino shape:\", X_img_dino.shape)\n",
        "print(\"X_tab_img_dino shape:\", X_tab_img_dino.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-VdXLYwWgOm",
        "outputId": "7d7ee852-3aed-44ce-e6e9-b7df73c46e36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_A: 20000 N_B: 20000\n",
            "X_tab shape: (20000, 1)\n",
            "X_img_dino shape: (20000, 384)\n",
            "X_tab_img_dino shape: (20000, 385)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Sanity check\n",
        "\n",
        "def sanity_check_world(world_data, z_thick, z_slant, name=\"A\"):\n",
        "    \"\"\"\n",
        "    Do sanity check for world (A/B):\n",
        "    1) Check relationship between T, Y, tau_true and thickness/slant\n",
        "    2) Check if slant is irrelevent in World A, and relevent in World B\n",
        "    3) Check overlap: logistic regression of propensity\n",
        "    \"\"\"\n",
        "    T = world_data[\"T\"].astype(float)\n",
        "    Y = world_data[\"Y\"].astype(float)\n",
        "    tau = world_data[\"tau\"].astype(float)\n",
        "\n",
        "    print(f\"\\n====== Sanity check for World {name} ======\")\n",
        "    print(\"N =\", len(T))\n",
        "\n",
        "    df_tmp = pd.DataFrame({\n",
        "        \"thick\": z_thick,\n",
        "        \"slant\": z_slant,\n",
        "        \"T\": T,\n",
        "        \"Y\": Y,\n",
        "        \"tau\": tau,\n",
        "    })\n",
        "    print(\"\\n[World\", name, \"] Correlation matrix:\")\n",
        "    print(df_tmp.corr().round(3))\n",
        "\n",
        "    X_2d = np.stack([z_thick, z_slant], axis=1)\n",
        "    lin_Y = LinearRegression().fit(X_2d, Y)\n",
        "    Y_pred = lin_Y.predict(X_2d)\n",
        "    r2_Y = r2_score(Y, Y_pred)\n",
        "\n",
        "    print(f\"\\n[World {name}] LinearRegression: Y ~ thick + slant\")\n",
        "    print(\"  coef_thick =\", lin_Y.coef_[0], \"  coef_slant =\", lin_Y.coef_[1])\n",
        "    print(\"  R^2 =\", r2_Y)\n",
        "\n",
        "    lin_tau = LinearRegression().fit(X_2d, tau)\n",
        "    tau_pred = lin_tau.predict(X_2d)\n",
        "    r2_tau = r2_score(tau, tau_pred)\n",
        "\n",
        "    print(f\"\\n[World {name}] LinearRegression: tau ~ thick + slant\")\n",
        "    print(\"  coef_thick =\", lin_tau.coef_[0], \"  coef_slant =\", lin_tau.coef_[1])\n",
        "    print(\"  R^2 =\", r2_tau)\n",
        "\n",
        "    logit = LogisticRegression(max_iter=1000).fit(X_2d, T)\n",
        "    ps = logit.predict_proba(X_2d)[:, 1]\n",
        "\n",
        "    print(f\"\\n[World {name}] LogisticRegression: T ~ thick + slant\")\n",
        "    print(\"  coef_thick =\", logit.coef_[0][0], \"  coef_slant =\", logit.coef_[0][1], \"  intercept =\", logit.intercept_[0])\n",
        "    print(\"  Propensity range: min =\", ps.min(), \" max =\", ps.max())\n",
        "\n",
        "    print(\"\\n====== End sanity check for World\", name, \"======\\n\")\n",
        "\n",
        "\n",
        "sanity_check_world(worldA, z_thick, z_slant, name=\"A\")\n",
        "sanity_check_world(worldB, z_thick, z_slant, name=\"B\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp3cpJ9caCJQ",
        "outputId": "14253456-e422-49f4-defe-3f8927b3f807"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Sanity check for World A ======\n",
            "N = 20000\n",
            "\n",
            "[World A ] Correlation matrix:\n",
            "       thick  slant      T      Y    tau\n",
            "thick  1.000  0.020  0.385  0.803  1.000\n",
            "slant  0.020  1.000  0.011  0.010  0.020\n",
            "T      0.385  0.011  1.000  0.412  0.385\n",
            "Y      0.803  0.010  0.412  1.000  0.803\n",
            "tau    1.000  0.020  0.385  0.803  1.000\n",
            "\n",
            "[World A] LinearRegression: Y ~ thick + slant\n",
            "  coef_thick = 1.415114   coef_slant = -0.01030814\n",
            "  R^2 = 0.6453971508026426\n",
            "\n",
            "[World A] LinearRegression: tau ~ thick + slant\n",
            "  coef_thick = 0.50000006   coef_slant = 2.110571e-08\n",
            "  R^2 = 0.9999999999999801\n",
            "\n",
            "[World A] LogisticRegression: T ~ thick + slant\n",
            "  coef_thick = 0.9775317621602353   coef_slant = 0.0006916068906344529   intercept = 0.010205897903427642\n",
            "  Propensity range: min = 0.0976339602633423  max = 0.99994712380043\n",
            "\n",
            "====== End sanity check for World A ======\n",
            "\n",
            "\n",
            "====== Sanity check for World B ======\n",
            "N = 20000\n",
            "\n",
            "[World B ] Correlation matrix:\n",
            "       thick  slant      T      Y    tau\n",
            "thick  1.000  0.020  0.287  0.630  0.714\n",
            "slant  0.020  1.000  0.444  0.621  0.714\n",
            "T      0.287  0.444  1.000  0.530  0.512\n",
            "Y      0.630  0.621  0.530  1.000  0.876\n",
            "tau    0.714  0.714  0.512  0.876  1.000\n",
            "\n",
            "[World B] LinearRegression: Y ~ thick + slant\n",
            "  coef_thick = 1.3593961   coef_slant = 1.3391155\n",
            "  R^2 = 0.7681405894223068\n",
            "\n",
            "[World B] LinearRegression: tau ~ thick + slant\n",
            "  coef_thick = 0.5000001   coef_slant = 0.50000006\n",
            "  R^2 = 0.9999999999999614\n",
            "\n",
            "[World B] LogisticRegression: T ~ thick + slant\n",
            "  coef_thick = 0.7912174594501729   coef_slant = 1.1838484501406388   intercept = -0.015497466936242047\n",
            "  Propensity range: min = 0.005838818215833565  max = 0.999236733663002\n",
            "\n",
            "====== End sanity check for World B ======\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Linear probe: DINO_emb → thickness / slant\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def linear_probe_feature(X_emb, target, name=\"slant\"):\n",
        "    reg = LinearRegression().fit(X_emb, target)\n",
        "    pred = reg.predict(X_emb)\n",
        "    r2 = r2_score(target, pred)\n",
        "    print(f\"[Linear probe] R^2(DINO_emb → {name}) = {r2:.4f}\")\n",
        "    return r2\n",
        "\n",
        "print(\"\\n========== Linear probe on DINO embeddings ==========\\n\")\n",
        "\n",
        "r2_slant = linear_probe_feature(X_img_dino, z_slant, name=\"slant\")\n",
        "r2_thick = linear_probe_feature(X_img_dino, z_thick, name=\"thickness\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdHZdfN5YZNV",
        "outputId": "1b8d7032-2928-4340-b1a5-4763fbb0f784"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Linear probe on DINO embeddings ==========\n",
            "\n",
            "[Linear probe] R^2(DINO_emb → slant) = 0.6582\n",
            "[Linear probe] R^2(DINO_emb → thickness) = 0.9707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Define metrics and pipeline\n",
        "\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    return float(np.sqrt(np.mean((a - b)**2)))\n",
        "\n",
        "def policy_metrics(tau_hat, tau_true, ks=(0.1, 0.2, 0.3)):\n",
        "    out = []\n",
        "    n = len(tau_hat)\n",
        "    order_hat  = np.argsort(-tau_hat)\n",
        "    order_true = np.argsort(-tau_true)\n",
        "    for k in ks:\n",
        "        m = max(1, int(round(k * n)))\n",
        "        gh = float(np.mean(tau_true[order_hat[:m]]))\n",
        "        go = float(np.mean(tau_true[order_true[:m]]))\n",
        "        regret = 1.0 - gh / (go + 1e-12)\n",
        "        out.append((k, gh, go, regret))\n",
        "    return out\n",
        "\n",
        "\n",
        "def eval_causalpfn_morpho(\n",
        "    Z,                 # input\n",
        "    T_all,             # treatment\n",
        "    Y_all,             # outcome\n",
        "    tau_true_all,      # true ITE\n",
        "    name=\"DINO_only\",  # Pipeline name\n",
        "    K=5,               # K-fold\n",
        "    pca_dim=None,\n",
        "    ids=None\n",
        "):\n",
        "    \"\"\"\n",
        "    CausalPFN evaluation function：\n",
        "    - Use KFold；\n",
        "    - Output CATE_R2、PEHE、ATE_hat、ATE_true、ATE_MAE；\n",
        "    - Output policy metrics（@10%, 20%, 30%）；\n",
        "    - Use ATEEstimator to get ATE estimation；\n",
        "    - Save cate_pred_xxx.csv and run_config_xxx.json。\n",
        "    \"\"\"\n",
        "\n",
        "    Z = np.asarray(Z, np.float32)\n",
        "    T_all = np.asarray(T_all, np.float32)\n",
        "    Y_all = np.asarray(Y_all, np.float32)\n",
        "    tau_true_all = np.asarray(tau_true_all, np.float32)\n",
        "\n",
        "    N, d = Z.shape\n",
        "    if ids is None:\n",
        "        ids = np.arange(N)\n",
        "\n",
        "    cate_pred = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "    kf = KFold(n_splits=K, shuffle=True, random_state=0)\n",
        "\n",
        "    for fold, (tr, va) in enumerate(kf.split(Z), start=1):\n",
        "        Z_tr, Z_va = Z[tr], Z[va]\n",
        "        T_tr, Y_tr = T_all[tr], Y_all[tr]\n",
        "        tau_va     = tau_true_all[va]\n",
        "\n",
        "        if pca_dim is not None and pca_dim < Z_tr.shape[1]:\n",
        "            pca = PCA(n_components=pca_dim, random_state=0).fit(Z_tr)\n",
        "            Ze_tr = pca.transform(Z_tr)\n",
        "            Ze_va = pca.transform(Z_va)\n",
        "        else:\n",
        "            Ze_tr, Ze_va = Z_tr, Z_va\n",
        "\n",
        "        sc = StandardScaler().fit(Ze_tr)\n",
        "        X_tr = sc.transform(Ze_tr).astype(np.float32)\n",
        "        X_va = sc.transform(Ze_va).astype(np.float32)\n",
        "\n",
        "        cate_est = CATEEstimator(device=device, verbose=False)\n",
        "        cate_est.fit(X_tr, T_tr, Y_tr)\n",
        "        cate_pred[va] = cate_est.estimate_cate(X_va).astype(np.float32)\n",
        "\n",
        "        print(f\"[{name}][Fold {fold}] Train={len(tr)}  Val={len(va)}  done.\")\n",
        "\n",
        "    cate_r2   = r2_score(tau_true_all, cate_pred)\n",
        "    pehe_rmse = rmse(cate_pred, tau_true_all)\n",
        "    ate_hat   = float(np.mean(cate_pred))\n",
        "    ate_true  = float(np.mean(tau_true_all))\n",
        "    ate_mae   = abs(ate_hat - ate_true)\n",
        "    pol       = policy_metrics(cate_pred, tau_true_all, ks=(0.10, 0.20, 0.30))\n",
        "\n",
        "    print(f\"[{name}] CATE_R2={cate_r2:.4f}  PEHE={pehe_rmse:.4f}  \"\n",
        "          f\"ATE_hat={ate_hat:.4f}  ATE_true={ate_true:.4f}  ATE_MAE={ate_mae:.4f}\")\n",
        "    for k,gh,go,rg in pol:\n",
        "        print(f\"[{name}][Policy@{int(k*100)}%] gain_hat={gh:.4f}  gain_oracle={go:.4f}  norm_regret={rg:.4f}\")\n",
        "\n",
        "    ate_est = ATEEstimator(device=device, verbose=False)\n",
        "    ate_est.fit(Z.astype(np.float32), T_all, Y_all)\n",
        "    ate_hat_global = float(ate_est.estimate_ate())\n",
        "    print(f\"[{name}][ATEEstimator] ATE_hat_global={ate_hat_global:.4f}\")\n",
        "\n",
        "    os.makedirs(\"outputs\", exist_ok=True)\n",
        "    out_df = pd.DataFrame({\n",
        "        \"id\": ids,\n",
        "        \"tau_hat\": cate_pred,\n",
        "        \"tau_true\": tau_true_all,\n",
        "        \"T\": T_all,\n",
        "        \"Y\": Y_all,\n",
        "    })\n",
        "    out_path = f\"outputs/cate_pred_{name}.csv\"\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "\n",
        "    run_cfg = {\n",
        "        \"name\": name,\n",
        "        \"n\": int(N),\n",
        "        \"d_in\": int(d),\n",
        "        \"K\": int(K),\n",
        "        \"pca_dim\": int(pca_dim) if pca_dim is not None else None,\n",
        "        \"metrics\": {\n",
        "            \"CATE_R2\": float(cate_r2),\n",
        "            \"PEHE_RMSE\": float(pehe_rmse),\n",
        "            \"ATE_hat\": float(ate_hat),\n",
        "            \"ATE_true\": float(ate_true),\n",
        "            \"ATE_MAE\": float(ate_mae),\n",
        "            \"Policy\": [\n",
        "                {\"k\": float(k), \"gain_hat\": float(gh), \"gain_oracle\": float(go), \"norm_regret\": float(rg)}\n",
        "                for (k,gh,go,rg) in pol\n",
        "            ],\n",
        "            \"ATEEstimator_global\": float(ate_hat_global)\n",
        "        }\n",
        "    }\n",
        "    cfg_path = f\"outputs/run_config_{name}.json\"\n",
        "    json.dump(run_cfg, open(cfg_path, \"w\"), indent=2)\n",
        "    print(\"Saved:\", out_path, \"and\", cfg_path)\n",
        "\n",
        "    return cate_pred, run_cfg"
      ],
      "metadata": {
        "id": "kfAQgX6IWkIN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Oracle baseline: Z = [thickness, slant]\n",
        "\n",
        "X_oracle = np.stack([z_thick, z_slant], axis=1).astype(np.float32)\n",
        "ids = np.arange(len(z_thick))\n",
        "\n",
        "print(\"\\n========== Oracle PFN with [thick, slant] ==========\\n\")\n",
        "\n",
        "# World A\n",
        "cate_A_oracle, cfg_A_oracle = eval_causalpfn_morpho(\n",
        "    Z=X_oracle,\n",
        "    T_all=worldA[\"T\"],\n",
        "    Y_all=worldA[\"Y\"],\n",
        "    tau_true_all=worldA[\"tau\"],\n",
        "    name=\"WorldA_oracle_thick_slant\",\n",
        "    K=5,\n",
        "    pca_dim=None,\n",
        "    ids=ids,\n",
        ")\n",
        "\n",
        "# World B\n",
        "cate_B_oracle, cfg_B_oracle = eval_causalpfn_morpho(\n",
        "    Z=X_oracle,\n",
        "    T_all=worldB[\"T\"],\n",
        "    Y_all=worldB[\"Y\"],\n",
        "    tau_true_all=worldB[\"tau\"],\n",
        "    name=\"WorldB_oracle_thick_slant\",\n",
        "    K=5,\n",
        "    pca_dim=None,\n",
        "    ids=ids,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "ce1ef95ff6404c67a349aeeee7a4ce8b",
            "4411dc84372e48a9a4aeaeace73a77a9",
            "b548def29f2a4c679ae5f9705538240f",
            "b4ceaa147c80489abb62d087b3b8da20",
            "13efc401930541a4b2a4235ab6b803be",
            "a1d138c722544103a33bbb6874eaf9ca",
            "5bb472a2cf2449baa93db89a88976116",
            "32adb7360c8b414782435949aae06c6e",
            "b0d054d0662e4719b32cb62c2cde97a6",
            "28f9df665d444a2cb07de67c2578d8e7",
            "c2272b43fe4a4af992dce5f068132f3f"
          ]
        },
        "id": "FkrqK7zea4gP",
        "outputId": "c82fc61b-dcee-4306-8f5d-79a849b61e59"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Oracle PFN with [thick, slant] ==========\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "causalpfn_v0.pt:   0%|          | 0.00/75.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce1ef95ff6404c67a349aeeee7a4ce8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WorldA_oracle_thick_slant][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldA_oracle_thick_slant][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldA_oracle_thick_slant][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldA_oracle_thick_slant][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldA_oracle_thick_slant][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldA_oracle_thick_slant] CATE_R2=0.9301  PEHE=0.1322  ATE_hat=0.5108  ATE_true=0.5000  ATE_MAE=0.0108\n",
            "[WorldA_oracle_thick_slant][Policy@10%] gain_hat=1.5270  gain_oracle=1.5352  norm_regret=0.0053\n",
            "[WorldA_oracle_thick_slant][Policy@20%] gain_hat=1.2420  gain_oracle=1.2651  norm_regret=0.0183\n",
            "[WorldA_oracle_thick_slant][Policy@30%] gain_hat=1.0679  gain_oracle=1.1001  norm_regret=0.0293\n",
            "[WorldA_oracle_thick_slant][ATEEstimator] ATE_hat_global=0.4996\n",
            "Saved: outputs/cate_pred_WorldA_oracle_thick_slant.csv and outputs/run_config_WorldA_oracle_thick_slant.json\n",
            "[WorldB_oracle_thick_slant][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldB_oracle_thick_slant][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldB_oracle_thick_slant][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldB_oracle_thick_slant][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldB_oracle_thick_slant][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldB_oracle_thick_slant] CATE_R2=0.9121  PEHE=0.2117  ATE_hat=0.5546  ATE_true=0.5000  ATE_MAE=0.0546\n",
            "[WorldB_oracle_thick_slant][Policy@10%] gain_hat=1.7762  gain_oracle=1.8103  norm_regret=0.0189\n",
            "[WorldB_oracle_thick_slant][Policy@20%] gain_hat=1.4961  gain_oracle=1.5283  norm_regret=0.0211\n",
            "[WorldB_oracle_thick_slant][Policy@30%] gain_hat=1.3201  gain_oracle=1.3422  norm_regret=0.0165\n",
            "[WorldB_oracle_thick_slant][ATEEstimator] ATE_hat_global=0.5420\n",
            "Saved: outputs/cate_pred_WorldB_oracle_thick_slant.csv and outputs/run_config_WorldB_oracle_thick_slant.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Fully evaluate World A / World B\n",
        "\n",
        "# World A：Image is redundant\n",
        "print(\"\\n========== World A: Image is redundant ==========\\n\")\n",
        "\n",
        "cate_A_tab, cfg_A_tab = eval_causalpfn_morpho(\n",
        "    Z=X_tab,              # (N, 1)\n",
        "    T_all=T_A,\n",
        "    Y_all=Y_A,\n",
        "    tau_true_all=tau_A,\n",
        "    name=\"WorldA_tab_only\",\n",
        "    K=5,\n",
        "    pca_dim=None,\n",
        "    ids=ids_A,\n",
        ")\n",
        "\n",
        "cate_A_img, cfg_A_img = eval_causalpfn_morpho(\n",
        "    Z=X_img_dino,\n",
        "    T_all=T_A,\n",
        "    Y_all=Y_A,\n",
        "    tau_true_all=tau_A,\n",
        "    name=\"WorldA_dino_only\",\n",
        "    K=5,\n",
        "    pca_dim=48,\n",
        "    ids=ids_A,\n",
        ")\n",
        "\n",
        "cate_A_tab_img, cfg_A_tab_img = eval_causalpfn_morpho(\n",
        "    Z=X_tab_img_dino,\n",
        "    T_all=T_A,\n",
        "    Y_all=Y_A,\n",
        "    tau_true_all=tau_A,\n",
        "    name=\"WorldA_tab_plus_dino\",\n",
        "    K=5,\n",
        "    pca_dim=48,\n",
        "    ids=ids_A,\n",
        ")\n",
        "\n",
        "\n",
        "#World B：Image has hidden confounder\n",
        "print(\"\\n========== World B: Image has hidden confounder ==========\\n\")\n",
        "\n",
        "cate_B_tab, cfg_B_tab = eval_causalpfn_morpho(\n",
        "    Z=X_tab,\n",
        "    T_all=T_B,\n",
        "    Y_all=Y_B,\n",
        "    tau_true_all=tau_B,\n",
        "    name=\"WorldB_tab_only\",\n",
        "    K=5,\n",
        "    pca_dim=None,\n",
        "    ids=ids_B,\n",
        ")\n",
        "\n",
        "cate_B_img, cfg_B_img = eval_causalpfn_morpho(\n",
        "    Z=X_img_dino,\n",
        "    T_all=T_B,\n",
        "    Y_all=Y_B,\n",
        "    tau_true_all=tau_B,\n",
        "    name=\"WorldB_dino_only\",\n",
        "    K=5,\n",
        "    pca_dim=48,\n",
        "    ids=ids_B,\n",
        ")\n",
        "\n",
        "cate_B_tab_img, cfg_B_tab_img = eval_causalpfn_morpho(\n",
        "    Z=X_tab_img_dino,\n",
        "    T_all=T_B,\n",
        "    Y_all=Y_B,\n",
        "    tau_true_all=tau_B,\n",
        "    name=\"WorldB_tab_plus_dino\",\n",
        "    K=5,\n",
        "    pca_dim=48,\n",
        "    ids=ids_B,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rVCF-ngXqjP",
        "outputId": "c8d04995-1609-49ab-da34-2c6da6f992ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== World A: Image is redundant ==========\n",
            "\n",
            "[WorldA_tab_only][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_only][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_only][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_only][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_only][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_only] CATE_R2=0.9681  PEHE=0.0893  ATE_hat=0.5120  ATE_true=0.5000  ATE_MAE=0.0120\n",
            "[WorldA_tab_only][Policy@10%] gain_hat=1.5338  gain_oracle=1.5352  norm_regret=0.0009\n",
            "[WorldA_tab_only][Policy@20%] gain_hat=1.2470  gain_oracle=1.2651  norm_regret=0.0143\n",
            "[WorldA_tab_only][Policy@30%] gain_hat=1.0855  gain_oracle=1.1001  norm_regret=0.0133\n",
            "[WorldA_tab_only][ATEEstimator] ATE_hat_global=0.5088\n",
            "Saved: outputs/cate_pred_WorldA_tab_only.csv and outputs/run_config_WorldA_tab_only.json\n",
            "[WorldA_dino_only][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldA_dino_only][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldA_dino_only][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldA_dino_only][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldA_dino_only][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldA_dino_only] CATE_R2=0.6399  PEHE=0.3000  ATE_hat=0.5813  ATE_true=0.5000  ATE_MAE=0.0813\n",
            "[WorldA_dino_only][Policy@10%] gain_hat=1.4176  gain_oracle=1.5352  norm_regret=0.0766\n",
            "[WorldA_dino_only][Policy@20%] gain_hat=1.1684  gain_oracle=1.2651  norm_regret=0.0764\n",
            "[WorldA_dino_only][Policy@30%] gain_hat=1.0174  gain_oracle=1.1001  norm_regret=0.0752\n",
            "[WorldA_dino_only][ATEEstimator] ATE_hat_global=0.5626\n",
            "Saved: outputs/cate_pred_WorldA_dino_only.csv and outputs/run_config_WorldA_dino_only.json\n",
            "[WorldA_tab_plus_dino][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_plus_dino][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_plus_dino][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_plus_dino][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_plus_dino][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldA_tab_plus_dino] CATE_R2=0.6376  PEHE=0.3010  ATE_hat=0.5825  ATE_true=0.5000  ATE_MAE=0.0825\n",
            "[WorldA_tab_plus_dino][Policy@10%] gain_hat=1.4231  gain_oracle=1.5352  norm_regret=0.0731\n",
            "[WorldA_tab_plus_dino][Policy@20%] gain_hat=1.1640  gain_oracle=1.2651  norm_regret=0.0799\n",
            "[WorldA_tab_plus_dino][Policy@30%] gain_hat=1.0149  gain_oracle=1.1001  norm_regret=0.0774\n",
            "[WorldA_tab_plus_dino][ATEEstimator] ATE_hat_global=0.5567\n",
            "Saved: outputs/cate_pred_WorldA_tab_plus_dino.csv and outputs/run_config_WorldA_tab_plus_dino.json\n",
            "\n",
            "========== World B: Image has hidden confounder ==========\n",
            "\n",
            "[WorldB_tab_only][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_only][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_only][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_only][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_only][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_only] CATE_R2=-2.6211  PEHE=1.3587  ATE_hat=1.7373  ATE_true=0.5000  ATE_MAE=1.2373\n",
            "[WorldB_tab_only][Policy@10%] gain_hat=1.3780  gain_oracle=1.8103  norm_regret=0.2388\n",
            "[WorldB_tab_only][Policy@20%] gain_hat=1.1423  gain_oracle=1.5283  norm_regret=0.2526\n",
            "[WorldB_tab_only][Policy@30%] gain_hat=1.0301  gain_oracle=1.3422  norm_regret=0.2325\n",
            "[WorldB_tab_only][ATEEstimator] ATE_hat_global=1.7159\n",
            "Saved: outputs/cate_pred_WorldB_tab_only.csv and outputs/run_config_WorldB_tab_only.json\n",
            "[WorldB_dino_only][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldB_dino_only][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldB_dino_only][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldB_dino_only][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldB_dino_only][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldB_dino_only] CATE_R2=-0.6900  PEHE=0.9282  ATE_hat=1.1933  ATE_true=0.5000  ATE_MAE=0.6933\n",
            "[WorldB_dino_only][Policy@10%] gain_hat=1.3620  gain_oracle=1.8103  norm_regret=0.2476\n",
            "[WorldB_dino_only][Policy@20%] gain_hat=1.1820  gain_oracle=1.5283  norm_regret=0.2266\n",
            "[WorldB_dino_only][Policy@30%] gain_hat=1.0494  gain_oracle=1.3422  norm_regret=0.2182\n",
            "[WorldB_dino_only][ATEEstimator] ATE_hat_global=1.1505\n",
            "Saved: outputs/cate_pred_WorldB_dino_only.csv and outputs/run_config_WorldB_dino_only.json\n",
            "[WorldB_tab_plus_dino][Fold 1] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_plus_dino][Fold 2] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_plus_dino][Fold 3] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_plus_dino][Fold 4] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_plus_dino][Fold 5] Train=16000  Val=4000  done.\n",
            "[WorldB_tab_plus_dino] CATE_R2=-0.6979  PEHE=0.9304  ATE_hat=1.1980  ATE_true=0.5000  ATE_MAE=0.6980\n",
            "[WorldB_tab_plus_dino][Policy@10%] gain_hat=1.3647  gain_oracle=1.8103  norm_regret=0.2462\n",
            "[WorldB_tab_plus_dino][Policy@20%] gain_hat=1.1817  gain_oracle=1.5283  norm_regret=0.2268\n",
            "[WorldB_tab_plus_dino][Policy@30%] gain_hat=1.0453  gain_oracle=1.3422  norm_regret=0.2212\n",
            "[WorldB_tab_plus_dino][ATEEstimator] ATE_hat_global=1.1413\n",
            "Saved: outputs/cate_pred_WorldB_tab_plus_dino.csv and outputs/run_config_WorldB_tab_plus_dino.json\n"
          ]
        }
      ]
    }
  ]
}
